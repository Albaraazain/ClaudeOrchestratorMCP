{"timestamp": "2026-01-13T11:55:57.409954", "agent_id": "race-condition-hunte-115400-cfc373", "finding_type": "issue", "severity": "high", "message": "lifecycle.py:990-1118 - NESTED LockedRegistryFile pattern still in use. Main lock at line 990, then NESTED locks at lines 1085 and 1100 for cleanup result updates. If cleanup fails, inner lock acquisition at 1100 can deadlock if same process already holds outer lock (reentrant lock scenario depends on fcntl implementation).", "data": {"file": "orchestrator/lifecycle.py", "lines": "990, 1085, 1100, 1118", "pattern": "nested_locks", "risk": "potential_deadlock"}}
{"timestamp": "2026-01-13T11:56:03.239898", "agent_id": "race-condition-hunte-115400-cfc373", "finding_type": "issue", "severity": "medium", "message": "health_daemon.py:788,837,897 - LockedRegistryFile still used for global registry updates. These JSON file locks are NOT migrated to SQLite despite the migration effort. Can cause lock contention when health daemon runs concurrently with MCP server operations.", "data": {"file": "orchestrator/health_daemon.py", "lines": "788, 837, 897", "pattern": "json_file_locks", "risk": "lock_contention"}}
{"timestamp": "2026-01-13T11:56:24.964569", "agent_id": "race-condition-hunte-115400-cfc373", "finding_type": "issue", "severity": "critical", "message": "TOCTOU Race Condition in deploy_claude_tmux_agent (real_mcp_server.py:1027-1033 vs 1155+): check_can_spawn_agent() checks constraints BUT returns BEFORE agent is inserted. Between check (line 1027-1033) and INSERT (via deploy_agent_atomic called later around line 1155+), another concurrent request can pass the same check. Both will think they can spawn, causing duplicate agents that violate max_concurrent/max_agents limits.", "data": {"file": "real_mcp_server.py + state_db.py", "pattern": "TOCTOU", "check_location": "real_mcp_server.py:1027-1033", "insert_location": "deploy_agent_atomic in state_db.py:2144-2190", "risk": "duplicate_agents_beyond_limits"}}
{"timestamp": "2026-01-13T11:56:53.114717", "agent_id": "race-condition-hunte-115400-cfc373", "finding_type": "issue", "severity": "high", "message": "TOCTOU in _maybe_auto_submit_phase_for_review_sqlite (real_mcp_server.py:3463-3488): Reads phase status at line 3474-3480 (get_phase), then updates status at 3484-3488 (update_phase_status). Race window exists where two concurrent agents completing could both read 'ACTIVE' status, both pass the check, and both try to update to 'AWAITING_REVIEW'. This can cause duplicate reviewer spawn attempts.", "data": {"file": "real_mcp_server.py", "function": "_maybe_auto_submit_phase_for_review_sqlite", "lines": "3463-3488", "pattern": "TOCTOU", "risk": "duplicate_review_submissions"}}
{"timestamp": "2026-01-13T11:57:05.424715", "agent_id": "race-condition-hunte-115400-cfc373", "finding_type": "issue", "severity": "high", "message": "Multiple paths calling _auto_spawn_phase_reviewers can race (real_mcp_server.py:3532-3542 + 3587-3594): Both the main path (line 3587) and the retry worker thread (line 3532) can call _auto_spawn_phase_reviewers. While claim_phase_for_review() prevents duplicate spawns WITHIN this function, the retry worker checks _maybe_auto_submit_phase_for_review_sqlite() first (which has its own race condition reported earlier). Multiple agents completing + retry workers = multiple code paths trying to transition/spawn.", "data": {"file": "real_mcp_server.py", "lines": "3532, 3587", "pattern": "multiple_spawn_paths", "risk": "concurrent_spawn_attempts"}}
{"timestamp": "2026-01-13T11:57:28.121182", "agent_id": "race-condition-hunte-115400-cfc373", "finding_type": "issue", "severity": "medium", "message": "list_real_tasks (real_mcp_server.py:3936-3937) reads GLOBAL_REGISTRY.json without any locking. Concurrent writes to this file (by health_daemon or lifecycle.py) can cause partial reads, resulting in JSON parse errors or inconsistent data. Should migrate to SQLite or use LockedRegistryFile for reads too.", "data": {"file": "real_mcp_server.py", "function": "list_real_tasks", "lines": "3936-3937", "pattern": "unlocked_read", "risk": "partial_read_corruption"}}
{"timestamp": "2026-01-13T11:58:05.630749", "agent_id": "race-condition-hunte-115400-cfc373", "finding_type": "issue", "severity": "medium", "message": "Potential race in submit_review_verdict (real_mcp_server.py:5271-5305): check_review_complete is called AFTER record_review_verdict's transaction commits. If two reviewers submit at same time, both could see is_complete=True and both could call finalize_review. While SQLite likely handles this via transactions, the finalize_review could be called twice. Should verify finalize_review is idempotent or add atomic claim mechanism like claim_phase_for_review.", "data": {"file": "real_mcp_server.py", "function": "submit_review_verdict", "lines": "5271-5305", "pattern": "double_finalization", "risk": "duplicate_phase_transitions"}}
{"timestamp": "2026-01-13T11:58:17.196329", "agent_id": "race-condition-hunte-115400-cfc373", "finding_type": "recommendation", "severity": "critical", "message": "RACE CONDITION FIX SUMMARY: (1) TOCTOU in deploy - merge check_can_spawn_agent and deploy_agent_atomic into single transaction with INSERT...SELECT WHERE; (2) TOCTOU in phase review - merge get_phase + update_phase_status into atomic claim like claim_phase_for_review; (3) NESTED LOCKS in lifecycle.py - refactor to single lock or migrate to SQLite; (4) GLOBAL REGISTRY - migrate remaining LockedRegistryFile usages in health_daemon.py to SQLite; (5) REVIEW FINALIZATION - add claim_review_finalization() to prevent double finalize.", "data": {"total_issues": 7, "critical": 1, "high": 4, "medium": 2, "fix_pattern": "Atomic check-and-modify operations using SQLite transactions or fcntl-based claims"}}
